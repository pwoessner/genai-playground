{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import GoogleSerperRun, WikipediaQueryRun\n",
    "from langchain.tools.google_scholar.tool import GoogleScholarQueryRun\n",
    "from langchain.utilities import GoogleSerperAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.utilities.google_scholar import GoogleScholarAPIWrapper\n",
    "\n",
    "from internals.llm import sapAI, ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/D064058/Code/04_PoCs/genai-playground/tools.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/D064058/Code/04_PoCs/genai-playground/tools.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m load_dotenv()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/D064058/Code/04_PoCs/genai-playground/tools.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m llm \u001b[39m=\u001b[39m sapAI()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "llm = sapAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GoogleSerperRun' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/D064058/Code/04_PoCs/genai-playground/tools.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/D064058/Code/04_PoCs/genai-playground/tools.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m serper \u001b[39m=\u001b[39m GoogleSerperRun(api_wrapper\u001b[39m=\u001b[39mGoogleSerperAPIWrapper())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/D064058/Code/04_PoCs/genai-playground/tools.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m scholar \u001b[39m=\u001b[39m GoogleScholarQueryRun(api_wrapper\u001b[39m=\u001b[39mGoogleScholarAPIWrapper())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/D064058/Code/04_PoCs/genai-playground/tools.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m wikipedia \u001b[39m=\u001b[39m WikipediaQueryRun(api_wrapper\u001b[39m=\u001b[39mWikipediaAPIWrapper())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GoogleSerperRun' is not defined"
     ]
    }
   ],
   "source": [
    "serper = GoogleSerperRun(api_wrapper=GoogleSerperAPIWrapper())\n",
    "scholar = GoogleScholarQueryRun(api_wrapper=GoogleScholarAPIWrapper())\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "tools = [scholar, wikipedia, serper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find information about a specific research paper.\n",
      "Action: google_scholar\n",
      "Action Input: ZARA: Improving Few-Shot Self-Rationalization for Small Language Models\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNo good Google Scholar Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe paper might be too new or not indexed on Google Scholar yet. I should try a general Google search.\n",
      "Action: google_serper\n",
      "Action Input: ZARA: Improving Few-Shot Self-Rationalization for Small Language Models key findings\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mInspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of ... ... findings-naacl.31; Corpus ID: 244130199. Few-Shot Self-Rationalization with ... ZARA: Improving Few-Shot Self-Rationalization for Small Language Models · Wei ... Abstract. Self-rationalization models that predict task labels and generate free-text elaborations for their predictions could enable more intuitive ... Missing: key | Show results with:key. ... self-training by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA ... Missing: key | Show results with:key. ... self-training by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on ... Missing: key | Show results with:key. ZARA: Improving Few-Shot Self-Rationalization for Small Language Models ... model using multimodal inputs to improve the rationality of generated results. Self-rationalization models that predict task labels and generate free-text elaborations for their predictions could enable more intuitive interaction with ... Missing: ZARA. Empirical results demonstrate that by simply prompting large language models trained ... ZARA: Improving Few-Shot Self-Rationalization for Small Language Models. “ZARA: Improving Few-Shot Self-Rationalization for Small Language Models.” In Proceedings of the 2023 Conference on Empirical Methods in Natural Language ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results indicate that the ZARA approach improves self-rationalization models by reducing the problem of plausibility judgement to natural language inference. It also mentions that ZARA achieves state-of-the-art performance, but it doesn't provide specific key findings.\n",
      "Action: google_serper\n",
      "Action Input: ZARA: Improving Few-Shot Self-Rationalization for Small Language Models abstract\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mInspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of ... Abstract. Self-rationalization models that predict task labels and generate ... We show there is still ample room for improvement in this task: the average ... Article “ZARA: Improving Few-Shot Self-Rationalization for Small Language Models” Detailed information of the J-GLOBAL is a service based on the concept of ... Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Missing: abstract | Show results with:abstract. [Show full abstract] we explore the less-studied setting of leveraging explanations for small LMs to improve few-shot self-rationalization. This work identifies the right prompting approach by extensively exploring natural language prompts on FEB and demonstrates that making progress on few-shot ... Missing: abstract | Show results with:abstract. Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Missing: abstract | Show results with:abstract. “ZARA: Improving Few-Shot Self-Rationalization for Small Language Models.” In Proceedings of the 2023 Conference on Empirical Methods in Natural Language ... ZARA: Improving Few-Shot Self-Rationalization for Small Language Models ... abstract and retrieve useful knowledge in the related work written in both English ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results provide some additional information about the ZARA approach. It seems to be a novel approach that leverages explanations for small language models to improve few-shot self-rationalization. It also identifies the right prompting approach by extensively exploring natural language prompts. However, the specific key findings are still not clear.\n",
      "Final Answer: The ZARA approach improves self-rationalization models by reducing the problem of plausibility judgement to natural language inference and achieves state-of-the-art performance. It leverages explanations for small language models to improve few-shot self-rationalization and identifies the right prompting approach by extensively exploring natural language prompts. However, the specific key findings are not clear from the available information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The ZARA approach improves self-rationalization models by reducing the problem of plausibility judgement to natural language inference and achieves state-of-the-art performance. It leverages explanations for small language models to improve few-shot self-rationalization and identifies the right prompting approach by extensively exploring natural language prompts. However, the specific key findings are not clear from the available information.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are the key findings in the paper Improving Few-Shot Self-Rationalization for Small Language Models\"\n",
    "response = agent_chain.run(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sapai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
